# What's Block Directly Architecture

Block Directly架构的目的是以尽量少的损耗，将存储硬件本该提供的性能能力提供给应用，同时具有ServerSAN的所有特性。
![架构图](images/arch.png?raw=true "BlockDirectly架构全图")

## 动机
我们都知道软件模块并不能创造性能，系统运行的性能源动力来自硬件能力。软件是以消耗性能为代价换取更加丰富的功能，比如更高的可靠性，thin provision能力等等。
软件工程的发展过程是一个抽象程度不断提升的过程，以编程语言为例，从机器码、汇编到C++，再到Java、Python,抽象能力不断提升，程序员的工作效率不断提高，但对硬件资源的利用效率却不断降低。
然而硬件的发展越来越接近物理极限，无论是芯片制程还是通信速率，一点点微弱的提高都要极大的研发付出和运行时的功耗成本。更可怕的是数据内容的产生速度并没有因此而减缓，数据中心在整个社会能耗的占比不断提升。
是时候对传统的云存储模型做出反思了。



## BlockDirectly架构要点
1. 在合适的层级引出合适的服务
一味的做减法简化设计并不能满足所有场景的存储需求。BlockDirectly架构通过层级化的方案，在合适的层级引出不同服务。如本文开头的总图，这些服务从Performance Volume到 NAS文件存储，提供的服务逐渐复杂丰富，软件栈也越来越厚。相反，在软件栈较薄的PureFlash层，提供语义最简单、性能最好的性能型SAN服务。

在系统核心部分只提供最基础的服务。那么哪些是最基础的服务？依次是：可靠性（多副本机制，故障处理）、集群管理、性能。这些是提供一个具有用户实用价值服务的最小子集。Performance Volume也就是基于这些服务。

2. RDMA的使用模型
大部分软件工程师都把RDMA当成一种网络技术来用，当然这的确是事实。然而仅仅把RDMA当成一种性能更好的TCP并不能充分发挥RDMA应有的价值。RDMA需要思维模式的改变。
直观上RDMA 相比TCP只是增加了READ/WRITE两个操作模式。沿袭TCP的使用习惯，也是为了兼容TCP，一种很常见的RDMA使用模式是这样的：当需要发送的数据量小于某个阈值时就使用SEND/RECV，超过阈值后使用READ/WRITE。在整个模型之上提供和TCP一样的recv/send接口，这样更上一层的RPC框架就可以顺畅的运行了。

在我们的架构里使用的时另外一种用法，一种更“硬件工程师”思维的用法。这种用法就像NVMe SSD使用DMA一样来使用RDMA。具体一点，SEND/RECV操作用于传输IO的comamnd 和completion。 数据部分则使用READ/WRITE传输，重要的一点是何时执行READ/WRITE是在IO流程执行过程中决定的，而不是网络层会贪婪的收取所有发个自己的数据。这样可以带来流控上的收益，包括网络流控以缓解RoCE RDMA网络拥塞有帮助。以及Server端处理能力的流控，Server可以决定何时处理哪个IO请求，以达到最优化使用自身资源，按规定满足客户端QoS。

关键路径上的RPC要简化。通用的RPC框架的功能都非常丰富，能满足任何通用的场合。通用RPC为满足通用场景必然准备了大量的丰富的功能，特别是每次RPC前后的序列化、反序列化 ，对性能都是极大的损耗。然而对于存储这样一个通用服务，调用频次非常之高、RPC类型非常之单一、性能要求非常之高。这种情况非常适合为其准备专用方法的。 同理，这里可以参考NVMe over Fabric协议，在二进制语义基础上定义高效的Client-Server协作协议，只有将不必要的损耗降到最低后RDMA带来的延迟降低才更弥足珍贵。

4. 线程模型
通常从软件角度看线程的设计是以任务需要为准，简单的例子就是在处理网络连接时有多少个连接就创建多少个线程，操作系统的多任务能力保证了每个线程都得到运行，也依赖操作系统将系统资源充分调度。
但是这个美好的假定在我们追求极致时就不那么正确了。操作系统的调度工作同样会花费大量的资源，每次线程切换都会导致TLB的失效，甚至是L3cache的失效。对于latency 要求在几十微妙的现代存储服务而言，这些成本都高到不能忽视。
那么正确的线程模型应该是什么样的呢？谈到这里想起两篇论文，《Why threads are a bad idea》《Whey events are a bad idea》，虽然这两篇论文都有些年头了，但是仍然代表着处理多任务时的两种相反极端。走向任何一个极端都是不对的，所有的选择都要对达到目标有利。

笔者认为正确的线程模型应该是以资源最大化为第一目标，其次兼顾编程便利性。

首先来说一下“资源”，资源包括两方面，一是CPU资源。现代的CPU都是多核的，一个核上同时只能执行一个线程，不同的核可以同时执行多个线程。为了避免线程调度开销，最理想的方法就是每个核上一个线程，这样就无需任何调度。
第二种资源是程序中的数据结构资源，比如一个保存了全局信息的一个hash表，多个线程来访问的话就要加锁处理，确保大家得到一致的结果。然而加锁的开销是非常大的，这也是我们要极力避免的。
举个例子，在PureFlash的PfFlashStore类里有个查找表`obj_lmt`，来自不同Dispatcher的IO可能会查询同一个PfFlashStore对象的obj_lmt，如果使用加锁的方法保持同步，那么这个查找表就会阻碍Dispatcher线程并行工作。
```
class PfFlashStore : public PfEventThread
{
	std::unordered_map<struct lmt_key, struct lmt_entry*, struct lmt_hash> obj_lmt; //act as lmt table in S5
};
```
PureFlash采取的方法是为每个PfFlashStore对象创建一个单独的贤臣，将跟PfFlashStore对象相关的操作都放到这个线程进行。在Dispatcher线程处理完毕后就把IO转给PfFlashStore线程处理。Dispather线程就可以继续处理其他IO请求而无需等待查找表的查找结果。

再说下“其次兼顾编程便利性”，对于非关键路径的线程完全可以回归到传统的思路，以方便代码逻辑的组织为目标。同步逻辑的关键在于等待机制，让程序依次执行，这样更容易理解，更容易调试。举个例子：
```
void handle_delete_replica()
{
    PfFlashStore* disk = app_context.trays[i];
	int rc = disk->sync_invoke([disk, rep_id]()->int{
		return disk->delete_replica(replica_id_t(rep_id));
	});
	send_reply_to_client(reply, nc);
}	
```
这里的`sync_invoke`将一个lambda表达式放到目标线程里面执行，这样lambda表达式内部操作就和目标线程在同一个线程而无需加锁操作。在目标线程完成lambda表达式前，调用线程将处于等待；对于调用线程这是一个顺序执行的过程，并没有因为线程切换将代码打的七零八落，仍然保持了很好的可读性与可维护性。而对于目标线程有避免了加锁和等待锁的过程，保证了目标线程的高效运行。
总的来说这样的一个机制就是确保快路径的高效执行，保持慢路径的可维护性。


5. 内存管理
内存管理简单讲就是内存的分配释放，也就是malloc/free这两个函数的使用。这两个函数接口非常简单没什么好说的，值得讨论的是性能。glibc的内存分配器虽然也在不断进步，然而还是有很多其他的分配器，比如jemalloc, tcmalloc, ptmalloc，有一个[论文](http://www.adms-conf.org/2019-camera-ready/durner_adms19.pdf) 做了比较。然而我们仍然希望进一步将内存分配器的开销降低。

一般而言内存分配器的工作包括两个步骤：一）调用系统系调用(sbrk, mmap)从系统的buddy系统分配到大块的内存；二）分配器用自己的管理算法将内存批发转零售分配给应用代码使用。在高性能存储系统里面，第二步的调用频率会非常的高，直接间接的调用每秒能达几百万次。任何微小的延迟增加，甚至每次分配多执行几条指令累计起来都是很大的影响。

PureFlash采用了这样的方法：
1) 一次性将存储系统将要使用的内存从OS全部申请出来，并且锁定到物理内存。这样也彻底避免了后期运行过程中OOM。
2) 内存按照单个IO的最大大小切成固定区域，并使用一个queu进行管理。这样所有的malloc操作就都简化成了一个dequeue操作，具体就是一个`head++`操作。从而最大化的简化了内存分配的开销。

这样做还有一个额外的好处，RDMA操作使用的内存是需要锁定在物理内存里的，这一点我们刚刚说过已经把所有内存全部锁定了。RDMA的内存在能够传递给post_xxxx API使用前还需要进行注册，ibv_reg_mr函数的性能也是非常差的，而且抖动厉害，在糟糕的情况下一次内存注册会消耗几十毫秒。我们使用固定内存分配就可以提前把内存注册到RDMA网卡，避免后面运行时的注册开销。

这样的设计读者很容易会有疑问，如此不就时限制了系统的弹性能力，无法最大化利用系统的内存资源。笔者的观点是这样的，一个系统的处理能力是其最短板决定的。如果一个系统已经达到了其CPU能力的瓶颈，此时仍然尽力从系统分配内存来承接client端发来是数据，无论是内存分配动作、还是数据接收操作都是要消耗CPU的，只会让系统本已紧张的CPU资源更加捉襟见肘，从而导致恶性循环。这样并没有增加系统的弹性能力，只是徒然浪费RAM和CPU资源。
笔者的观点是，系统的最大性能是有极限的，我们只需要准备足够极限性能条件下使用的内存资源就可以了。
那么系统极限性能条件下使用的内存资源怎么确定呢？可以通过下面的公式计算：

![1](https://latex.codecogs.com/svg.latex?\\M=K\times%20B\times%20L "K*B*L")

其中， K系统的最大性能，iops ； B是系统支持的最大单IO大小； L 是在系统达到最大性能时的延迟（以秒为单位）。

例如如果一个系统设计目标是100万iops, 允许最大的单个IO是64KB，并且经过测试知道在100万iops时的平均延迟是1ms。那么这个系统需要预留的内存就是

![1](https://latex.codecogs.com/svg.image?(1\times&space;10^{6})\times&space;64\textit{kB}&space;\times&space;(1\times&space;10^{-3})&space;=&space;64\textit{MB})

这个数字可能出乎大部分人的预料，为了支撑100万iops的IO需要的内存竟然这么少。当然实际的系统要预留足够的缓冲能力，以应付随机发生的意外情况导致系统延迟恶化。这里的关键就又回到了我们的网络模型，系统对于输入请求要量能力而入，而不是来者不拒。系统要选择、要控制优先级决定服务哪些IO请求。这样看似延迟了服务，却保障了系统整体的效率最高。

这里另外解释一下上面的公式计算为什么选择最大IO大小为64KB，当前在云计算环境大家常见的物美价廉的RDMA方案是RoCEv2, RoCEv2使用UDP作为下层传输协议，而UDP的最大包大小是64KB. 如果我们把IO的最大大小也限制在64KB,就能最高效的利用网络传输效率，将网络拥塞时的惩罚降到最小。



6. 基于AOF的EC方案
   可以参加文件![AOF架构](./aof.md "AOF架构")
7. 基于LSM tree的文件系统
   LSM tree 有很多优势，在rocksdb为代表的应用中得到了充分证明。LSM tree在文件系统要求的随机小块IO和块设备提供的大块IO之间架起了一座桥梁。
   
   LSM tree的一个重要特性在于其对底层存储的要求， 只需要底层提供append only操作，即AOF即可。
   文件系统的架构是个大课题，请参加文件系统架构设计文档。
   
8. 适应ZNS, SMR
   ZNS, SMR是近些年存储介质领域出现的新技术。能够极大提升介质的存储密度。这些技术把存储的复杂服务（随机IO访问）延迟到后续的服务层提供，less is more， 以提供更多的可能性。这点和PureFlash系统的思想是一致的。
   
   具体的，AOF的特性和ZNS, SMR的追加写能力完全吻合。通过AOF作为中间层我们可以在存储成本、存储功能之间二者兼顾。