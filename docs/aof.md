Append only File (AOF)
======================

## 1. 为什么要提供AOF功能
作为存储最通用的功能，通常我们认为他有随机访问能力。无论是块存储还是文件存储，都可以指定offset来访问数据。
所有的应用，操作系统设计时都是这么假设存储系统的。
然而随机访问这个feature 对存储引擎高级功能带来了实现上的困难。这里的高级功能特指：EC, 压缩。
比如EC，当一个满条带落盘后，想要更新其中的一个数据块就很难了，需要将整个满条带数据读回啦，然后替换，重新计算校验。
这会带来非常大的写放大。

AOF文件特性单一，不提供随机访问，因此对于EC, 压缩这样的功能就很友好。
同时，AOF向上可以承载LSMT这样的数据结构，为进一步承载像rocks DB这样的业务提供了可能。

## 2. AOF 在PureFlash里的实现机制
每个AOF文件就是一个Volume。未防止Volume client和AOF client误操作破坏数据，在元数据部分增加一个

### 2.1 多副本AOF文件实现机制
多副本AOF文件的数据使用多副本方式保护数据可靠性。
多副本卷是PureFlash最基本的功能，也是性能最好的访问方式。实现AOF文件需要：
1）数据追加写在上次写过的尾部。那么我们用一个卷作为文件，每次写就像Volume写一样，只是写在尾部而不是覆盖写。
2) 记录文件的最新长度, 每次追加写后都要更新文件长度。这属于元数据。
文件长度这个元数据保存在哪里合适？如果像Volume的元数据一样保存在数据库里，那么数据库就会成为IO路径的一部分，而我们对数据库
的定位是低频volume元数据访问，承担不了这么高频的文件长度元数据。只有把长度保存在Volume数据部分才能满足高性能要求。

方法1：
在Volume的头部保留一个4K空间作为元数据区。
然后由Client API写入数据时对元数据区的length进行更新。
这个方法逻辑比较简单，现在的实现选择此方法。

方法2：
在每个shard保留第一个4K，在这里记录file length. 这样就是，每次append write后，PureFlash server端负责写入length。
这个方法的复杂之处在于 server需要构造一个4K的buffer，然后把长度填入，然后下发到盘。
这里当然可以复用IO发过来时就带的buffer，在EVT_IO_COMPLETE 时再次下发IO, 这种方式需要dispatcher下发两次IO。
如果使用io_uring的Linked IO，只需要下发一次IO, 但需要一次性要把buffer都准备好，无法复用client 发过来的buffer。

   
### 2.2 EC AOF文件实现机制
   每次append都要补满一个满条带，然后写入。如果不是满条带，就补0计算。但是文件的长度可以是任意数，不需要是满条带长度的倍数。


   SSD硬件加速点：
   SSD 提供block remap功能， 不用搬移数据，就实现逻辑上的移动。
   比如4+2EC, 在EC前的积累阶段用多副本模式写入：
```   
   Addr     1  2  3
------------------------
   disk1:   D1 D2 D3
   disk2:   D1 D2 D3
   disk3:   D1 D2 D3
```   
   当第4个数据块D4写入后，可以使用EC模式分布。Primary要求数据分布变成：
```
   disk1:   D1
   disk2:   D2
   disk3:   D3
   disk4:   D4
   disk5:   P5
   disk6:   P6
```
   对于disk2, 需要删掉D1, D2, D3，然后重新把D2写入到原来D1的位置，即位置1。
   这就很糟糕，增加SSD的无效数据移动，浪费了SSD寿命。
   remap直接把D2重新映射到D1的LBA位置，又快又经济


3. 基于AOF的EC volume 



4. 基于AOF的RocksDB

6. 基于RocksDB的General File API